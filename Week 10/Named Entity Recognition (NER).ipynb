{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dad6e04",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69a22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d123e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 459ms/step - accuracy: 0.2111 - loss: 1.9431 - val_accuracy: 0.3000 - val_loss: 1.9352\n",
      "Epoch 2/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.4833 - loss: 1.9165 - val_accuracy: 0.4000 - val_loss: 1.9191\n",
      "Epoch 3/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5444 - loss: 1.8901 - val_accuracy: 0.4000 - val_loss: 1.8998\n",
      "Epoch 4/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6000 - loss: 1.8543 - val_accuracy: 0.4000 - val_loss: 1.8756\n",
      "Epoch 5/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5611 - loss: 1.8102 - val_accuracy: 0.5000 - val_loss: 1.8448\n",
      "Epoch 6/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6611 - loss: 1.7481 - val_accuracy: 0.5000 - val_loss: 1.8067\n",
      "Epoch 7/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6611 - loss: 1.6673 - val_accuracy: 0.5000 - val_loss: 1.7646\n",
      "Epoch 8/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6611 - loss: 1.5317 - val_accuracy: 0.5000 - val_loss: 1.7409\n",
      "Epoch 9/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6611 - loss: 1.4116 - val_accuracy: 0.5000 - val_loss: 1.8111\n",
      "Epoch 10/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.6611 - loss: 1.2745 - val_accuracy: 0.5000 - val_loss: 1.9504\n",
      "Epoch 11/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6833 - loss: 1.3137 - val_accuracy: 0.5000 - val_loss: 1.9039\n",
      "Epoch 12/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6556 - loss: 1.0830 - val_accuracy: 0.6000 - val_loss: 1.7578\n",
      "Epoch 13/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6556 - loss: 1.0187 - val_accuracy: 0.6000 - val_loss: 1.6036\n",
      "Epoch 14/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6167 - loss: 0.9809 - val_accuracy: 0.6000 - val_loss: 1.5062\n",
      "Epoch 15/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6167 - loss: 0.9382 - val_accuracy: 0.6000 - val_loss: 1.4596\n",
      "Epoch 16/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.6000 - loss: 1.0065 - val_accuracy: 0.5000 - val_loss: 1.4445\n",
      "Epoch 17/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6778 - loss: 0.8870 - val_accuracy: 0.5000 - val_loss: 1.4557\n",
      "Epoch 18/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6778 - loss: 0.8881 - val_accuracy: 0.5000 - val_loss: 1.4682\n",
      "Epoch 19/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6833 - loss: 0.9703 - val_accuracy: 0.5000 - val_loss: 1.4774\n",
      "Epoch 20/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6778 - loss: 0.9152 - val_accuracy: 0.5000 - val_loss: 1.4981\n",
      "Epoch 21/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7000 - loss: 0.9154 - val_accuracy: 0.5000 - val_loss: 1.5327\n",
      "Epoch 22/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6778 - loss: 0.8003 - val_accuracy: 0.5000 - val_loss: 1.5828\n",
      "Epoch 23/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6444 - loss: 0.8786 - val_accuracy: 0.5000 - val_loss: 1.6259\n",
      "Epoch 24/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6444 - loss: 0.8670 - val_accuracy: 0.5000 - val_loss: 1.6668\n",
      "Epoch 25/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6833 - loss: 0.8277 - val_accuracy: 0.5000 - val_loss: 1.6934\n",
      "Epoch 26/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7167 - loss: 0.7458 - val_accuracy: 0.5000 - val_loss: 1.7041\n",
      "Epoch 27/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6444 - loss: 0.8108 - val_accuracy: 0.5000 - val_loss: 1.6791\n",
      "Epoch 28/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7167 - loss: 0.7738 - val_accuracy: 0.5000 - val_loss: 1.6606\n",
      "Epoch 29/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6833 - loss: 0.8558 - val_accuracy: 0.5000 - val_loss: 1.6176\n",
      "Epoch 30/30\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7167 - loss: 0.6644 - val_accuracy: 0.5000 - val_loss: 1.5961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 1: Synthetic Dataset\n",
    "# --------------------------------------------\n",
    "sentences = [\n",
    "    [\"John\", \"works\", \"at\", \"Google\", \"in\", \"New\", \"York\", \".\"],\n",
    "    [\"Microsoft\", \"is\", \"based\", \"in\", \"Seattle\", \",\", \"Washington\", \".\"],\n",
    "    [\"Elon\", \"Musk\", \"founded\", \"SpaceX\", \"and\", \"Tesla\", \".\"],\n",
    "    [\"Berlin\", \"is\", \"the\", \"capital\", \"of\", \"Germany\", \".\"]\n",
    "]\n",
    "\n",
    "original_tags = [\n",
    "    [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"I-LOC\", \"O\"],\n",
    "    [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\", \"O\"],\n",
    "    [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"O\", \"B-ORG\", \"O\"],\n",
    "    [\"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"]\n",
    "]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 2: Preprocessing\n",
    "# --------------------------------------------\n",
    "# Create word-to-index mapping\n",
    "words = list(set(word for sentence in sentences for word in sentence))\n",
    "words.append(\"<PAD>\")  # Padding token\n",
    "words.append(\"<UNK>\")  # Unknown token\n",
    "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# Create tag-to-index mapping\n",
    "unique_tags = list(set(tag for tags in original_tags for tag in tags))\n",
    "unique_tags.append(\"<PAD>\")  # Padding tag\n",
    "tag_to_idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "\n",
    "# Convert sentences and tags to indices\n",
    "X = [[word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in sentence] for sentence in sentences]\n",
    "y = [[tag_to_idx[tag] for tag in tags] for tags in original_tags]\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_length = 10\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length, padding=\"post\", value=word_to_idx[\"<PAD>\"])\n",
    "y_padded = pad_sequences(y, maxlen=max_seq_length, padding=\"post\", value=tag_to_idx[\"<PAD>\"])\n",
    "\n",
    "# Reshape y for training\n",
    "y_padded = y_padded.reshape(*y_padded.shape, 1)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_padded, test_size=0.2)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 3: Build LSTM Model\n",
    "# --------------------------------------------\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=len(word_to_idx), \n",
    "        output_dim=64, \n",
    "        input_length=max_seq_length\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.TimeDistributed(layers.Dense(len(unique_tags), activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 4: Train the Model\n",
    "# --------------------------------------------\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=2,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 5: Predict on New Sentences\n",
    "# --------------------------------------------\n",
    "def predict_entities(sentence):\n",
    "    # Convert sentence to indices\n",
    "    sentence_indices = [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in sentence]\n",
    "    padded_sentence = pad_sequences(\n",
    "        [sentence_indices], \n",
    "        maxlen=max_seq_length, \n",
    "        padding=\"post\", \n",
    "        value=word_to_idx[\"<PAD>\"]\n",
    "    )\n",
    "    # Predict tags\n",
    "    predictions = model.predict(padded_sentence)\n",
    "    predicted_indices = np.argmax(predictions, axis=-1)[0]\n",
    "    # Map indices to tags\n",
    "    idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "    predicted_tags = [idx_to_tag[idx] for idx in predicted_indices if idx != tag_to_idx[\"<PAD>\"]]\n",
    "    return predicted_tags[:len(sentence)]  # Remove padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef861e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test Example\n",
    "test_sentence = [\"Tim\", \"Cook\", \"is\", \"CEO\", \"of\", \"Apple\", \"in\", \"California\", \".\"]\n",
    "predicted_tags = predict_entities(test_sentence)\n",
    "print(\"Predicted Tags:\", predicted_tags)\n",
    "# Output Example: [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c0558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d2d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f081097",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"John\", \"works\", \"at\", \"Google\", \"in\", \"New\", \"York\"],\n",
    "    [\"Apple\", \"is\", \"based\", \"in\", \"Cupertino\", \",\", \"California\"],\n",
    "    [\"Elon\", \"Musk\", \"leads\", \"Tesla\", \"and\", \"SpaceX\"]\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"I-LOC\"],\n",
    "    [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\"],\n",
    "    [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"O\", \"B-ORG\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21aa0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and tag-to-index mappings\n",
    "words = list(set(word for sentence in sentences for word in sentence))\n",
    "tags = list(set(tag for sentence_tags in tags for tag in sentence_tags))\n",
    "\n",
    "word_to_idx = {word: idx+1 for idx, word in enumerate(words)}  # +1 for padding\n",
    "tag_to_idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "\n",
    "# Add padding token\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "tag_to_idx[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7182d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"John\", \"works\", \"at\", \"Google\", \"in\", \"New\", \"York\"],\n",
    "    [\"Apple\", \"is\", \"based\", \"in\", \"Cupertino\", \",\", \"California\"],\n",
    "    [\"Elon\", \"Musk\", \"leads\", \"Tesla\", \"and\", \"SpaceX\"]\n",
    "]\n",
    "\n",
    "original_tag_sequences = [\n",
    "    [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"I-LOC\"],\n",
    "    [\"B-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"B-LOC\"],\n",
    "    [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"O\", \"B-ORG\"]\n",
    "]\n",
    "\n",
    "# 1. Create word-to-index mapping\n",
    "words = list(set(word for sentence in sentences for word in sentence))\n",
    "word_to_idx = {word: idx+1 for idx, word in enumerate(words)}\n",
    "word_to_idx[\"<PAD>\"] = 0\n",
    "\n",
    "# 2. Create tag-to-index mapping (use a new variable `unique_tags`)\n",
    "unique_tags = list(set(tag for sentence_tags in original_tag_sequences for tag in sentence_tags))\n",
    "tag_to_idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "tag_to_idx[\"<PAD>\"] = 0\n",
    "\n",
    "# 3. Convert sentences and tags to indices\n",
    "X = [[word_to_idx[word] for word in sentence] for sentence in sentences]\n",
    "y = [[tag_to_idx[tag] for tag in sentence_tags] for sentence_tags in original_tag_sequences]\n",
    "\n",
    "# 4. Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_padded = pad_sequences(X, maxlen=10, padding=\"post\", value=word_to_idx[\"<PAD>\"])\n",
    "y_padded = pad_sequences(y, maxlen=10, padding=\"post\", value=tag_to_idx[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285fde1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 5 5 4 5 4 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_padded[1])  # Output: [1, 5, 5, 2, 5, 3, 4, 0, 0, 0]\n",
    "# Corresponding to: [B-PER, O, O, B-ORG, O, B-LOC, I-LOC, <PAD>, <PAD>, <PAD>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11738b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
