{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2e23db",
   "metadata": {},
   "source": [
    "# WEEK-9 and Day-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4684b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362fbdc",
   "metadata": {},
   "source": [
    "# What is Gensim?\n",
    "Gensim is a Python library for topic modeling and vector space modeling.\n",
    "\n",
    "It offers efficient implementations of Word2Vec, FastText, Doc2Vec, etc.\n",
    "\n",
    "Simple API to train embeddings on your own text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c412f85",
   "metadata": {},
   "source": [
    "# 1. Word2Vec with Skip-gram (default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ba820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc12939b",
   "metadata": {},
   "source": [
    "# Explanation of parameters:\n",
    "vector_size=100: Dimension of word vectors.\n",
    "\n",
    "window=2: Context window size.\n",
    "\n",
    "min_count=1: Ignores words with frequency < 1.\n",
    "\n",
    "sg=0: CBOW; sg=1: Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9227ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
    "    ['the', 'dog', 'lay', 'on', 'the', 'rug'],\n",
    "    ['cats', 'and', 'dogs', 'are', 'friendly'],\n",
    "    ['the', 'mat', 'was', 'clean'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeb7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat': [ 1.63362399e-02 -8.88606533e-03  1.79708675e-02  1.65073294e-02\n",
      " -8.87044426e-03  6.06210204e-04  8.54898244e-03 -7.85264000e-03\n",
      " -1.11199310e-02 -1.30246449e-02 -1.34147645e-03 -5.91843156e-04\n",
      "  8.92616995e-03 -4.94810799e-03 -3.45218170e-04  4.92375158e-03\n",
      "  9.73519776e-03 -6.16168982e-05 -1.26788188e-02 -1.85216144e-02\n",
      "  5.33151615e-05  1.33237885e-02  2.93204538e-03 -1.79330446e-02\n",
      " -1.58772096e-02  1.31038046e-02 -7.57136103e-03  1.25099849e-02\n",
      " -1.33620640e-02  1.69593245e-02 -1.30326487e-02  6.57603983e-03\n",
      " -2.11397163e-03 -1.35750556e-02 -6.57519326e-03 -2.32282397e-03\n",
      " -1.09418798e-02 -2.42269505e-03 -1.51266269e-02  5.29331900e-03\n",
      "  1.81402974e-02 -4.75450046e-03 -1.95302011e-03  7.02712312e-03\n",
      "  1.73301753e-02 -1.18437055e-02 -1.37751559e-02 -5.86596970e-03\n",
      "  1.82953924e-02  1.73253531e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec Skip-gram model\n",
    "model_skipgram = Word2Vec(sentences, vector_size=50, window=2, sg=1, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "vector_cat = model_skipgram.wv['cat']\n",
    "print(\"Vector for 'cat':\", vector_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e98b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'cat': [('lay', 0.529292106628418), ('the', 0.21057099103927612), ('dog', 0.18857727944850922), ('cats', 0.1184762641787529), ('friendly', 0.08196330815553665), ('sat', 0.054149746894836426), ('on', 0.032278481870889664), ('mat', 0.011398468166589737), ('and', -0.02864326536655426), ('was', -0.03169109672307968)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find most similar words to 'cat'\n",
    "similar_to_cat = model_skipgram.wv.most_similar('cat')\n",
    "print(\"Words similar to 'cat':\", similar_to_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46805a44",
   "metadata": {},
   "source": [
    "# 2. Word2Vec with CBOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c33e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'dog': [ 1.62666347e-02 -8.91709048e-03 -2.13546003e-03  2.01451406e-03\n",
      " -3.84048908e-04  2.29703332e-03  1.22307539e-02 -4.19738135e-05\n",
      " -6.49385853e-03 -3.02319089e-03  1.17973071e-02  3.02837207e-03\n",
      " -1.44696282e-03  1.86667908e-02 -9.84282698e-03 -1.67649181e-03\n",
      "  1.83527451e-02  1.35006001e-02  3.00461403e-03 -1.77685898e-02\n",
      "  2.29815370e-03 -4.57581179e-03  1.87397823e-02  2.42115860e-03\n",
      "  2.98027229e-03  4.81483387e-03 -3.67410807e-03 -9.99960583e-03\n",
      "  4.64953861e-04 -4.02803440e-03  1.32045243e-02  1.78807322e-02\n",
      " -1.34860049e-03  5.95283601e-03 -1.22166462e-02  3.39736277e-03\n",
      " -1.38547607e-02 -1.73880141e-02 -1.18018985e-02 -1.79141499e-02\n",
      "  1.45582929e-02 -1.15435217e-02  1.65527854e-02 -1.44890593e-02\n",
      "  6.84585515e-03  1.93528291e-02 -1.55727053e-02 -1.98936667e-02\n",
      " -8.65858328e-03 -5.36802551e-03]\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec CBOW model\n",
    "model_cbow = Word2Vec(sentences, vector_size=50, window=2, sg=0, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "vector_dog = model_cbow.wv['dog']\n",
    "print(\"Vector for 'dog':\", vector_dog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87fdf2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'dog': [('clean', 0.1900876760482788), ('cat', 0.18863457441329956), ('lay', 0.1746498942375183), ('dogs', 0.11534516513347626), ('friendly', 0.10160074383020401), ('mat', 0.08059876412153244), ('the', 0.0406954251229763), ('was', -0.02331441454589367), ('are', -0.029120517894625664), ('on', -0.03338085860013962)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find most similar words to 'dog'\n",
    "similar_to_dog = model_cbow.wv.most_similar('dog')\n",
    "print(\"Words similar to 'dog':\", similar_to_dog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c35ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow.save(\"cbow_model.model\")\n",
    "loaded_model = Word2Vec.load(\"cbow_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40965948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim and Custom Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175ad5b",
   "metadata": {},
   "source": [
    "# Gensim and Custom Embedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e4934",
   "metadata": {},
   "source": [
    "# Why custom training?\n",
    "To understand the mechanics of Word2Vec.\n",
    "\n",
    "To customize loss functions or architectures.\n",
    "\n",
    "Useful for research or educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467dd82",
   "metadata": {},
   "source": [
    "# Basic Approach\n",
    "Implement CBOW or Skip-gram neural network manually.\n",
    "\n",
    "Use a simple corpus, one-hot inputs.\n",
    "\n",
    "Train embeddings as part of the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76afe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sentences = [\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "    ['deep', 'learning', 'improves', 'performance'],\n",
    "    ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'ai'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c847114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Qadri\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine learning is fun. Deep learning improves performance. Natural language processing is a field of AI.\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fdda19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Qadri\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbb348b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning is fun.',\n",
       " 'deep learning improves performance.',\n",
       " 'natural language processing is a field of ai.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split into sentences\n",
    "sentences = sent_tokenize(text.lower())\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c434872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['machine', 'learning', 'is', 'fun', '.'], ['deep', 'learning', 'improves', 'performance', '.'], ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'ai', '.']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7606dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom embadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a3af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the model on your tokenized sentences\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=3, min_count=1, sg=1)\n",
    "\n",
    "# Save your model for later use\n",
    "model.save(\"custom_word2vec.model\")\n",
    "\n",
    "# Load your model back\n",
    "model = Word2Vec.load(\"custom_word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "668ab08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning': [ 9.5819050e-05  3.0777829e-03 -6.8122493e-03 -1.3749062e-03\n",
      "  7.6691248e-03  7.3457342e-03 -3.6734061e-03  2.6435086e-03\n",
      " -8.3170552e-03  6.2051006e-03 -4.6356223e-03 -3.1636197e-03\n",
      "  9.3121473e-03  8.7397068e-04  7.4914210e-03 -6.0737138e-03\n",
      "  5.1616840e-03  9.9229561e-03 -8.4576169e-03 -5.1366789e-03\n",
      " -7.0642284e-03 -4.8634391e-03 -3.7776672e-03 -8.5366694e-03\n",
      "  7.9549942e-03 -4.8436727e-03  8.4232371e-03  5.2636852e-03\n",
      " -6.5498385e-03  3.9574602e-03  5.4696766e-03 -7.4259867e-03\n",
      " -7.4050669e-03 -2.4745166e-03 -8.6252997e-03 -1.5817145e-03\n",
      " -4.0349492e-04  3.2985571e-03  1.4400900e-03 -8.8140101e-04\n",
      " -5.5949753e-03  1.7308013e-03 -8.9678325e-04  6.7943181e-03\n",
      "  3.9738500e-03  4.5295926e-03  1.4344674e-03 -2.7008310e-03\n",
      " -4.3673716e-03 -1.0317086e-03  1.4368279e-03 -2.6458406e-03\n",
      " -7.0750611e-03 -7.8060934e-03 -9.1217179e-03 -5.9359740e-03\n",
      " -1.8485602e-03 -4.3232832e-03 -6.4599533e-03 -3.7171855e-03\n",
      "  4.2882399e-03 -3.7391903e-03  8.3782431e-03  1.5344918e-03\n",
      " -7.2433967e-03  9.4341561e-03  7.6310472e-03  5.4936577e-03\n",
      " -6.8490533e-03  5.8229882e-03  4.0090811e-03  5.1861429e-03\n",
      "  4.2549875e-03  1.9405761e-03 -3.1695992e-03  8.3536645e-03\n",
      "  9.6129002e-03  3.7930289e-03 -2.8375546e-03  6.9078310e-06\n",
      "  1.2182042e-03 -8.4569752e-03 -8.2242675e-03 -2.2970805e-04\n",
      "  1.2368216e-03 -5.7430118e-03 -4.7243480e-03 -7.3469253e-03\n",
      "  8.3288774e-03  1.2115674e-04 -4.5098569e-03  5.7007847e-03\n",
      "  9.1801435e-03 -4.0993984e-03  7.9646474e-03  5.3746752e-03\n",
      "  5.8799046e-03  5.1226612e-04  8.2126493e-03 -7.0189051e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the model: get vector for a word\n",
    "vector = model.wv['learning']\n",
    "print(\"Vector for 'learning':\", vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d3b56ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'learning': [('language', 0.1991359293460846), ('improves', 0.17276595532894135), ('of', 0.17016719281673431), ('a', 0.1459495723247528), ('field', 0.064043790102005), ('deep', 0.04654809460043907), ('processing', -0.002737886505201459), ('ai', -0.013365983963012695), ('is', -0.02367963083088398), ('natural', -0.03284512832760811)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find words most similar to 'learning'\n",
    "similar_words = model.wv.most_similar('learning')\n",
    "print(\"Words similar to 'learning':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642ae13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d21ae697",
   "metadata": {},
   "source": [
    "# DAY - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c6231",
   "metadata": {},
   "source": [
    "# Sequence Models in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1455120",
   "metadata": {},
   "source": [
    "# One-to-One Model or 1-to-1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2ec04",
   "metadata": {},
   "source": [
    "* Input: Single word (encoded)\n",
    "* Output: Single label\n",
    "\n",
    "* Task: Classify a single word into categories (e.g., positive or negative sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cab05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1753560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['good', 'bad', 'happy', 'sad', 'excellent', 'terrible']\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d697a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Because input is a single word, sequences are single integers\n",
    "X = np.array(sequences).squeeze()  # shape (6,)\n",
    "y = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e77c0a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NaVTTC AI Course\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e17bfa9450>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1to1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_1to1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_1to1.summary()\n",
    "\n",
    "model_1to1.fit(X, y, epochs=20, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5e7dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001E17D3F1C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction for 'excellent': 0.5158\n"
     ]
    }
   ],
   "source": [
    "test_word = ['excellent']\n",
    "test_seq = tokenizer.texts_to_sequences(test_word)\n",
    "pred = model_1to1.predict(np.array(test_seq))\n",
    "print(f\"Prediction for '{test_word[0]}': {pred[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde737a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba063ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NaVTTC AI Course\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e16dcb9290>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: word index and label (0 or 1)\n",
    "X = np.array([1, 2])\n",
    "y = np.array([1, 0])\n",
    "\n",
    "vocab_size = 5\n",
    "embedding_dim = 10\n",
    "output_dim = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d777e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predicted class for word 2: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "test_input = np.array([2])\n",
    "pred = model.predict(test_input)\n",
    "print(\"Predicted class for word 2:\", np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb99ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ca6606",
   "metadata": {},
   "source": [
    "# One-to-Many"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3109fb2",
   "metadata": {},
   "source": [
    "* Input: Single vector\n",
    "* Output: Sequence of words\n",
    "\n",
    "* Task: Generate a sequence given a fixed input vector\n",
    "* (Simplified toy example; here input is a fixed vector and output is a sequence of word indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef6d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 2.2866\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.2811\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 2.2755\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.2699\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.2641\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.2582\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 2.2522\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 2.2460\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.2395\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 2.2329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e1703baad0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_units = 16\n",
    "seq_length = 4\n",
    "\n",
    "inputs = tf.keras.Input(shape=(1,))          # Input shape (batch, 1)\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)  # (batch, 1, embed_dim)\n",
    "x = tf.keras.layers.Flatten()(x)              # now (batch, embed_dim)\n",
    "x = tf.keras.layers.RepeatVector(seq_length)(x)  # (batch, seq_length, embed_dim)\n",
    "x = tf.keras.layers.SimpleRNN(hidden_units, return_sequences=True)(x)\n",
    "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Toy data\n",
    "X = np.array([[1]])\n",
    "y = np.array([[[2], [3], [4], [5]]])  # shape (1, 4, 1)\n",
    "\n",
    "model.fit(X, y, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb8b5ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "Predicted sequence: [2 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "pred = model.predict(X)\n",
    "pred_seq = np.argmax(pred, axis=-1)[0]\n",
    "print(\"Predicted sequence:\", pred_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4183d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464888c7",
   "metadata": {},
   "source": [
    "# Many-to-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f03f99ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e171756290>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_units = 16\n",
    "output_dim = 2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(3,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = tf.keras.layers.SimpleRNN(hidden_units)(x)\n",
    "outputs = tf.keras.layers.Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Toy data: sequences of length 3 and labels\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([1, 0])\n",
    "\n",
    "model.fit(X, y, epochs=200, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70016b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Predicted class for sequence [1,2,3]: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "test_seq = np.array([[1, 2, 3]])\n",
    "pred = model.predict(test_seq)\n",
    "print(\"Predicted class for sequence [1,2,3]:\", np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4b5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7cc1a49",
   "metadata": {},
   "source": [
    "# Many-to-Many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7a7f69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e17176a090>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 8\n",
    "hidden_units = 16\n",
    "seq_length = 3\n",
    "\n",
    "inputs = tf.keras.Input(shape=(seq_length,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = tf.keras.layers.SimpleRNN(hidden_units, return_sequences=True)(x)\n",
    "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Toy data: input sequences and target sequences (same length)\n",
    "X = np.array([[1, 2, 3]])\n",
    "y = np.array([[[4], [5], [6]]])  # shape (1, 3, 1)\n",
    "\n",
    "model.fit(X, y, epochs=200, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f61f9991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "Predicted output sequence: [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "pred = model.predict(X)\n",
    "pred_seq = np.argmax(pred, axis=-1)[0]\n",
    "print(\"Predicted output sequence:\", pred_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4545c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb320e1a",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "542f1796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001E175567C40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 20\n",
    "embedding_dim = 16\n",
    "hidden_units = 32\n",
    "seq_length = 5\n",
    "output_dim = 2  # e.g., sentiment classes\n",
    "\n",
    "# Define model\n",
    "inputs = tf.keras.Input(shape=(seq_length,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_units))(x)\n",
    "outputs = tf.keras.layers.Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Toy data: 2 sequences and their labels\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 0]\n",
    "])\n",
    "y = np.array([1, 0])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Test prediction\n",
    "test_seq = np.array([[1, 2, 3, 4, 5]])\n",
    "pred = model.predict(test_seq)\n",
    "print(\"Predicted class:\", np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f718d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04c3fdf1",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a3de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\NaVTTC AI Course\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6930\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6000 - loss: 0.6915\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6900\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6884\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6869\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6000 - loss: 0.6854\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6837\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6000 - loss: 0.6821\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6803\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6000 - loss: 0.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e17bef1210>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, time_steps, features)\n",
    "        score = tf.matmul(inputs, self.W)  # (batch, time_steps, 1)\n",
    "        score = tf.squeeze(score, axis=-1)  # (batch, time_steps)\n",
    "        weights = tf.nn.softmax(score, axis=1)  # attention weights\n",
    "        weights = tf.expand_dims(weights, axis=-1)  # (batch, time_steps, 1)\n",
    "        context_vector = tf.reduce_sum(inputs * weights, axis=1)  # weighted sum\n",
    "        return context_vector, weights\n",
    "\n",
    "# Example model using attention for Many-to-One\n",
    "\n",
    "vocab_size = 15\n",
    "embedding_dim = 16\n",
    "rnn_units = 32\n",
    "seq_length = 7\n",
    "output_dim = 2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(seq_length,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True))(x)\n",
    "\n",
    "attention_layer = SimpleAttention()\n",
    "context_vector, attention_weights = attention_layer(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(output_dim, activation='softmax')(context_vector)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Toy data\n",
    "X = np.random.randint(0, vocab_size, size=(10, seq_length))\n",
    "y = np.random.randint(0, output_dim, size=(10,))\n",
    "\n",
    "model.fit(X, y, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04f68234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: [0.14269856 0.14318708 0.14304686 0.14286818 0.14282514 0.14275005\n",
      " 0.14262405]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict and get attention weights\n",
    "sample_input = np.random.randint(0, vocab_size, size=(1, seq_length))\n",
    "context, weights = attention_layer(model.layers[2](model.layers[1](sample_input)))\n",
    "print(\"Attention weights:\", weights.numpy().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf8d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
