{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32455688",
   "metadata": {},
   "source": [
    "# Install Libraries\n",
    "\n",
    "- pip install spacy nltk scikit-learn matplotlib\n",
    "- python -m nltk.downloader all\n",
    "- python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a99571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "adfc7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32decb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj\n",
      "love ROOT\n",
      "programming xcomp\n",
      "in prep\n",
      "Python pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I love programming in Python.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc8caeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj\n",
      "love ROOT\n",
      "programming xcomp\n",
      "in prep\n",
      "Python pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"I love programming in Python.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33d820e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benefit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "synonyms = wordnet.synsets(\"good\")\n",
    "print(synonyms[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "115871a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8225613236427307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silicon computers\\AppData\\Local\\Temp\\ipykernel_3244\\12575290.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1.similarity(doc2))\n"
     ]
    }
   ],
   "source": [
    "#Word similarity (SpaCy)\n",
    "doc1 = nlp(\"I have a cat.\")\n",
    "doc2 = nlp(\"I own a feline.\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6cc179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a polite request.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Can you open the window?\"\n",
    "# Pragmatically, it's a request, not a yes/no question\n",
    "print(\"This is a polite request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81104186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn on the fan.\n"
     ]
    }
   ],
   "source": [
    "context = {\"weather\": \"hot\"}\n",
    "if context[\"weather\"] == \"hot\":\n",
    "    print(\"Turn on the fan.\")  # In real NLP, AI infers actions based on context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "811d0510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I lost my keys.\n",
      "I found them in my bag.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I lost my keys.\", \"I found them in my bag.\"]\n",
    "for s in sentences:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d21eb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He = John\n"
     ]
    }
   ],
   "source": [
    "# Simple co-reference\n",
    "text = \"John went to the market. He bought some fruits.\"\n",
    "print(\"He = John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "709ce186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'NLP', 'AI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords with NLTK\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "words = [\"I\", \"am\", \"learning\", \"NLP\", \"and\", \"AI\"]\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "821ea2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Are you there\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import string\n",
    "\n",
    "text = \"Hello!!! Are you there??\"\n",
    "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a694efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d139ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "text = \"HELLO WORLD\"\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67206923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is awesome!\n"
     ]
    }
   ],
   "source": [
    "text = \"     NLP is awesome!     \"\n",
    "print(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62a085d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Email me at example@gmail.com\"\n",
    "email = re.findall(r'\\S+@\\S+', text)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b41e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$40']\n"
     ]
    }
   ],
   "source": [
    "text = \"The price is $40\"\n",
    "price = re.findall(r'\\$\\d+', text)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "910c926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "will AUX\n",
      "travel VERB\n",
      "tomorrow NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will travel tomorrow.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01db029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59671e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('playing', 'VBG'), ('football', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "sentence = \"He is playing football.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(pos_tag(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7cb1da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "will AUX\n",
      "travel VERB\n",
      "tomorrow NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I will travel tomorrow.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19e2dfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "## 13. Named Entity Recognition (NER)\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb0e028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "the United States GPE\n"
     ]
    }
   ],
   "source": [
    "# Another\n",
    "text = nlp(\"Barack Obama was the president of the United States.\")\n",
    "for entity in text.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "952adee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Chunking and Chinking\n",
    "\n",
    "## 14. Chunking\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"The\", \"DT\"), (\"big\", \"JJ\"), (\"dog\", \"NN\")]\n",
    "tree = cp.parse(sentence)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fb384f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "text = \"The quick brown fox jumps\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "grammar = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = RegexpParser(grammar)\n",
    "\n",
    "tree = cp.parse(tagged)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc2a0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk a different way\n",
    "grammar = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(\"The quick brown fox jumps\"))\n",
    "tree = cp.parse(tagged)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "51d9b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "## 15. Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "60e57977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a machine for performing calculations automatically\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 16. WordNet\n",
    "\n",
    "synonyms = wordnet.synsets(\"computer\")\n",
    "print(synonyms[0].definition())\n",
    "\n",
    "print(synonyms[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a749f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'is' 'love' 'nlp']\n",
      "[[0 0 1 1]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# üéØ Words as Features (Bag of Words Model)\n",
    "\n",
    "## 17. BoW Example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"I love NLP\", \"NLP is amazing\"])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d35248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\"AI is smart\", \"Machine learning is part of AI\"]\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6652d127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'future' 'is' 'like' 'the']\n"
     ]
    }
   ],
   "source": [
    "# üìà Feature Selection and Extraction\n",
    "\n",
    "## 18. Feature Selection\n",
    "\n",
    "# Using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform([\"I like AI\", \"AI is the future\"])\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33d94ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1., 1., 1.]), array([0.31731051, 0.31731051, 0.31731051]))\n"
     ]
    }
   ],
   "source": [
    "# Chi-square example (Advanced example)\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 1, 0], [1, 0, 1]])\n",
    "y = np.array([0, 1])\n",
    "chi_scores = chi2(X, y)\n",
    "print(chi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "513b71b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8707045912742615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silicon computers\\AppData\\Local\\Temp\\ipykernel_3244\\3088311006.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1.similarity(doc2))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üß† Document Similarity\n",
    "\n",
    "## 19. Document Similarity Example\n",
    "\n",
    "doc1 = nlp(\"Cats are beautiful animals.\")\n",
    "doc2 = nlp(\"Dogs are wonderful pets.\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4548ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.26055567]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cosine similarity manually\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [\"I love AI\", \"AI loves me\"]\n",
    "tfidf_matrix = TfidfVectorizer().fit_transform(texts)\n",
    "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ea32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4a00526f",
   "metadata": {},
   "source": [
    "pip install spacy nltk scikit-learn matplotlib\n",
    "python -m nltk.downloader all\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86396122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "#import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk, word_tokenize, RegexpParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d87e4fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Silicon\n",
      "[nltk_data]     computers\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1. Syntax\n",
      "Word: She, Dependency: nsubj, Head: reading\n",
      "Word: is, Dependency: aux, Head: reading\n",
      "Word: reading, Dependency: ROOT, Head: reading\n",
      "Word: a, Dependency: det, Head: book\n",
      "Word: book, Dependency: dobj, Head: reading\n",
      "Word: ., Dependency: punct, Head: reading\n",
      "[('She', 'PRP'), ('is', 'VBZ'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.')]\n",
      "\n",
      "# 2. Semantics\n",
      "Similarity: 0.832474946975708\n",
      "WordNet Synonym: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "# 3. Pragmatics\n",
      "Can you pass me the salt? ‚Üí This is a polite request.\n",
      "Take an umbrella.\n",
      "\n",
      "# 4. Discourse\n",
      "She went to the market.\n",
      "She bought some fruits.\n",
      "He went to the park. He played football. ‚Üí Linking 'He' across sentences.\n",
      "\n",
      "# 6. Tokenization\n",
      "NLTK Tokens: ['I', 'love', 'programming', '!']\n",
      "spaCy Tokens: ['I', 'enjoy', 'machine', 'learning', '.']\n",
      "\n",
      "# 7. Noise Removal\n",
      "Without Stopwords: ['example', 'sentence', '.']\n",
      "Without Punctuation: ['Hello', 'How', 'are', 'you', 'doing']\n",
      "\n",
      "# 8. Word & Sentence Tokenization\n",
      "spaCy Sentence Tokens: ['This is a sentence.', \"Here's another one.\"]\n",
      "NLTK Word Tokens: ['NLTK', 'makes', 'tokenization', 'easy', '.']\n",
      "\n",
      "# 9. Word Segmentation\n",
      "spaCy Word Segmentation: ['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "NLTK Word Segmentation: ['Natural', 'Language', 'Processing', 'is', 'awesome', '.']\n",
      "\n",
      "# 10. Stemming\n",
      "Stemmed 'running': run\n",
      "Stemmed 'flies': fli\n",
      "\n",
      "# 11. Text Normalization\n",
      "Lowercased: this is a sample text.\n",
      "Trimmed Spaces: NLP is amazing!\n",
      "\n",
      "# 12. Regular Expressions\n",
      "Emails found: ['example@email.com']\n",
      "Price found: ['$49.99']\n",
      "\n",
      "# 13. POS Tagging\n",
      "spaCy POS Tags:\n",
      "The: DET\n",
      "cat: NOUN\n",
      "is: AUX\n",
      "sitting: VERB\n",
      "on: ADP\n",
      "the: DET\n",
      "mat: NOUN\n",
      ".: PUNCT\n",
      "NLTK POS Tags: [('He', 'PRP'), ('is', 'VBZ'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('.', '.')]\n",
      "\n",
      "# 14. Named Entity Recognition (NER)\n",
      "spaCy NER:\n",
      "Barack Obama PERSON\n",
      "Hawaii GPE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Silicon computers\\AppData\\Local\\Temp\\ipykernel_3244\\3850953290.py:37: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(\"Similarity:\", doc2.similarity(doc3))  # Requires word vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK NER Tree (chunked): (S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  president/NN\n",
      "  ./.)\n",
      "\n",
      "# 15. Chunking and Chinking\n",
      "NLTK Chunk Tree (textual output): (S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN)\n",
      "  ./.)\n",
      "spaCy Noun Chunks:\n",
      "The quick brown fox\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# üìö All-in-One NLP Demonstration (Updated for Latest spaCy & NLTK Versions)\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "#import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk, word_tokenize, RegexpParser\n",
    "\n",
    "# ‚úÖ Setup for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# ‚úÖ Setup for spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"# 1. Syntax\")\n",
    "doc1 = nlp(\"She is reading a book.\")\n",
    "for token in doc1:\n",
    "    print(f\"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}\")\n",
    "print(nltk.pos_tag(nltk.word_tokenize(\"She is reading a book.\")))\n",
    "\n",
    "print(\"\\n# 2. Semantics\")\n",
    "doc2 = nlp(\"I love machine learning.\")\n",
    "doc3 = nlp(\"I enjoy AI research.\")\n",
    "try:\n",
    "    print(\"Similarity:\", doc2.similarity(doc3))  # Requires word vectors\n",
    "except:\n",
    "    print(\"Similarity: ‚ö†Ô∏è Word vectors not available in 'en_core_web_sm'\")\n",
    "print(\"WordNet Synonym:\", wordnet.synsets(\"car\")[0].definition())\n",
    "\n",
    "print(\"\\n# 3. Pragmatics\")\n",
    "print(\"Can you pass me the salt? ‚Üí This is a polite request.\")\n",
    "context = {\"weather\": \"rainy\"}\n",
    "if context[\"weather\"] == \"rainy\":\n",
    "    print(\"Take an umbrella.\")\n",
    "\n",
    "print(\"\\n# 4. Discourse\")\n",
    "sents = [\"She went to the market.\", \"She bought some fruits.\"]\n",
    "for s in sents:\n",
    "    print(s)\n",
    "print(\"He went to the park. He played football. ‚Üí Linking 'He' across sentences.\")\n",
    "\n",
    "print(\"\\n# 6. Tokenization\")\n",
    "tokens_nltk = nltk.word_tokenize(\"I love programming!\")\n",
    "print(\"NLTK Tokens:\", tokens_nltk)\n",
    "doc4 = nlp(\"I enjoy machine learning.\")\n",
    "print(\"spaCy Tokens:\", [token.text for token in doc4])\n",
    "\n",
    "print(\"\\n# 7. Noise Removal\")\n",
    "tokens = nltk.word_tokenize(\"This is an example sentence.\")\n",
    "filtered = [w for w in tokens if w.lower() not in stopwords.words('english')]\n",
    "print(\"Without Stopwords:\", filtered)\n",
    "doc5 = nlp(\"Hello!!! How are you doing?\")\n",
    "cleaned = [token.text for token in doc5 if not token.is_punct]\n",
    "print(\"Without Punctuation:\", cleaned)\n",
    "\n",
    "print(\"\\n# 8. Word & Sentence Tokenization\")\n",
    "doc6 = nlp(\"This is a sentence. Here's another one.\")\n",
    "print(\"spaCy Sentence Tokens:\", [sent.text for sent in doc6.sents])\n",
    "print(\"NLTK Word Tokens:\", nltk.word_tokenize(\"NLTK makes tokenization easy.\"))\n",
    "\n",
    "print(\"\\n# 9. Word Segmentation\")\n",
    "doc7 = nlp(\"I am learning Natural Language Processing.\")\n",
    "print(\"spaCy Word Segmentation:\", [token.text for token in doc7])\n",
    "print(\"NLTK Word Segmentation:\", nltk.word_tokenize(\"Natural Language Processing is awesome.\"))\n",
    "\n",
    "print(\"\\n# 10. Stemming\")\n",
    "stemmer = PorterStemmer()\n",
    "print(\"Stemmed 'running':\", stemmer.stem(\"running\"))\n",
    "print(\"Stemmed 'flies':\", stemmer.stem(\"flies\"))\n",
    "\n",
    "print(\"\\n# 11. Text Normalization\")\n",
    "print(\"Lowercased:\", \"THIS IS A SAMPLE TEXT.\".lower())\n",
    "print(\"Trimmed Spaces:\", \"    NLP is amazing!    \".strip())\n",
    "\n",
    "print(\"\\n# 12. Regular Expressions\")\n",
    "text1 = \"Contact me at example@email.com for details.\"\n",
    "emails = re.findall(r'\\b\\S+@\\S+\\b', text1)\n",
    "print(\"Emails found:\", emails)\n",
    "text2 = \"The total price is $49.99.\"\n",
    "price = re.findall(r'\\$\\d+\\.\\d{2}', text2)\n",
    "print(\"Price found:\", price)\n",
    "\n",
    "print(\"\\n# 13. POS Tagging\")\n",
    "doc8 = nlp(\"The cat is sitting on the mat.\")\n",
    "print(\"spaCy POS Tags:\")\n",
    "for token in doc8:\n",
    "    print(f\"{token.text}: {token.pos_}\")\n",
    "tokens = nltk.word_tokenize(\"He is reading a book.\")\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(\"NLTK POS Tags:\", tags)\n",
    "\n",
    "print(\"\\n# 14. Named Entity Recognition (NER)\")\n",
    "doc9 = nlp(\"Barack Obama was born in Hawaii.\")\n",
    "print(\"spaCy NER:\")\n",
    "for ent in doc9.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "sentence = \"Barack Obama was the president.\"\n",
    "tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "print(\"NLTK NER Tree (chunked):\", tree)\n",
    "\n",
    "print(\"\\n# 15. Chunking and Chinking\")\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = RegexpParser(grammar)\n",
    "tree = cp.parse(tags)\n",
    "print(\"NLTK Chunk Tree (textual output):\", tree)\n",
    "\n",
    "doc10 = nlp(sentence)\n",
    "print(\"spaCy Noun Chunks:\")\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf076bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
